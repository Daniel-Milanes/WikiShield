{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a078830a",
   "metadata": {
    "id": "a078830a"
   },
   "source": [
    "## Model performance results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c17206",
   "metadata": {
    "id": "90c17206"
   },
   "source": [
    "Calculate consistent set of metrics for results:\n",
    "\n",
    " - Show confusion matrix heatmap\n",
    " - Show classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53d990d",
   "metadata": {},
   "source": [
    "All models are tuned and use all of the features. We record their average accuracy score and F1 score over a 5-fold stratified cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5309cb",
   "metadata": {
    "id": "ff5309cb"
   },
   "source": [
    "### Baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bbbfe1",
   "metadata": {
    "id": "c3bbbfe1"
   },
   "source": [
    "Dummy random guessing classifier (blind guessing with equal probabilities): average accuracy score = 0.4982, average F1 score = 0.4915  (Note that our train dataset is nearly balanced)\n",
    "\n",
    "Logistic regression: average accuracy score = 0.8360, average F1 score = 0.8395\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea724c2",
   "metadata": {
    "id": "3ea724c2"
   },
   "source": [
    "### Other models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaf7ec4",
   "metadata": {
    "id": "cdaf7ec4"
   },
   "source": [
    "Decision tree: average accuracy score = 0.9014, average F1 score = 0.8970\n",
    "\n",
    "Random forest: average accuracy score = 0.8999, average F1 score = 0.8957\n",
    "\n",
    "Extra tree: average accuracy score = 0.8263, average F1 score = 0.7943\n",
    "\n",
    "Support Vector Classifier: average accuracy score = 0.8486, average F1 score = 0.8487"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b734aace",
   "metadata": {
    "id": "b734aace"
   },
   "source": [
    "### Flagship models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cee09f",
   "metadata": {
    "id": "27cee09f"
   },
   "source": [
    "### Comparison of models against baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388f1f92",
   "metadata": {
    "id": "388f1f92",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Testing results on the balanced and imbalanced test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc72d571",
   "metadata": {
    "id": "fc72d571"
   },
   "source": [
    "### Comparison of flagship against Cluebot's reported performance (just look at trial-reports maybe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fbd041",
   "metadata": {
    "id": "21fbd041"
   },
   "source": [
    "## Conclusion/Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb33dad9",
   "metadata": {
    "id": "fb33dad9"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{"cells":[{"cell_type":"markdown","id":"a078830a","metadata":{"id":"a078830a"},"source":["## Model performance results"]},{"cell_type":"markdown","id":"90c17206","metadata":{"id":"90c17206"},"source":["Calculate consistent set of metrics for results:\n","\n"," - Show confusion matrix heatmap\n"," - Show classification_report"]},{"cell_type":"markdown","id":"ff5309cb","metadata":{"id":"ff5309cb"},"source":["### Baseline models"]},{"cell_type":"markdown","id":"1f9314ff","metadata":{"id":"1f9314ff"},"source":["#### Before feature engineering"]},{"cell_type":"markdown","id":"b656563c","metadata":{"id":"b656563c"},"source":["#### After feature engineering, without vandalism_score"]},{"cell_type":"markdown","id":"c3bbbfe1","metadata":{"id":"c3bbbfe1"},"source":["#### With vandalism_score\n","\n","Logistic regression: average accuracy score = 0.8360, average F1 score = 0.8395\n"]},{"cell_type":"markdown","id":"3ea724c2","metadata":{"id":"3ea724c2"},"source":["### Other models"]},{"cell_type":"markdown","id":"55e5171d","metadata":{"id":"55e5171d"},"source":["#### Before feature engineering"]},{"cell_type":"markdown","id":"f00eb183","metadata":{"id":"f00eb183"},"source":["#### After feature engineering, without vandalism_score"]},{"cell_type":"markdown","id":"cdaf7ec4","metadata":{"id":"cdaf7ec4"},"source":["#### With vandalism_score\n","\n","Decision tree: average accuracy score = 0.9014, average F1 score = 0.8970\n","\n","Random forest: average accuracy score = 0.8999, average F1 score = 0.8957\n","\n","Extra tree: average accuracy score = 0.8263, average F1 score = 0.7943\n","\n","Support Vector Classifier: average accuracy score = 0.8486, average F1 score = 0.8487"]},{"cell_type":"markdown","id":"b734aace","metadata":{"id":"b734aace"},"source":["### Flagship models"]},{"cell_type":"markdown","id":"cf2a0c05","metadata":{"id":"cf2a0c05"},"source":["#### Before feature engineering"]},{"cell_type":"markdown","id":"21cf212b","metadata":{"id":"21cf212b"},"source":["#### After feature engineering, without vandalism_score"]},{"cell_type":"markdown","id":"0ef5ac88","metadata":{"id":"0ef5ac88"},"source":["#### With vandalism_score"]},{"cell_type":"markdown","id":"27cee09f","metadata":{"id":"27cee09f"},"source":["### Comparison of models against baselines"]},{"cell_type":"markdown","id":"388f1f92","metadata":{"vscode":{"languageId":"plaintext"},"id":"388f1f92"},"source":["### Testing results on the balanced and imbalanced test sets"]},{"cell_type":"markdown","id":"fc72d571","metadata":{"id":"fc72d571"},"source":["### Comparison of flagship against Cluebot's reported performance (just look at trial-reports maybe)"]},{"cell_type":"markdown","id":"21fbd041","metadata":{"id":"21fbd041"},"source":["## Conclusion/Insights"]},{"cell_type":"markdown","id":"fb33dad9","metadata":{"id":"fb33dad9"},"source":[]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}
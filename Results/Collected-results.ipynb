{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a078830a",
   "metadata": {},
   "source": [
    "## Model performance results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c17206",
   "metadata": {},
   "source": [
    "Calculate consistent set of metrics for results:\n",
    "\n",
    " - Show confusion matrix heatmap\n",
    " - Show classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5309cb",
   "metadata": {},
   "source": [
    "### Baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9314ff",
   "metadata": {},
   "source": [
    "#### Before feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b656563c",
   "metadata": {},
   "source": [
    "#### After feature engineering, without vandalism_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bbbfe1",
   "metadata": {},
   "source": [
    "#### With vandalism_score\n",
    "\n",
    "Logistic regression: average accuracy score = 0.8360, average F1 score = 0.8395\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea724c2",
   "metadata": {},
   "source": [
    "### Other models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e5171d",
   "metadata": {},
   "source": [
    "#### Before feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00eb183",
   "metadata": {},
   "source": [
    "#### After feature engineering, without vandalism_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaf7ec4",
   "metadata": {},
   "source": [
    "#### With vandalism_score\n",
    "\n",
    "Decision tree: average accuracy score = 0.9014, average F1 score = 0.8970\n",
    "\n",
    "Random forest: average accuracy score = 0.8999, average F1 score = 0.8957\n",
    "\n",
    "Extra tree: average accuracy score = 0.8263, average F1 score = 0.7943\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b734aace",
   "metadata": {},
   "source": [
    "### Flagship models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2a0c05",
   "metadata": {},
   "source": [
    "#### Before feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cf212b",
   "metadata": {},
   "source": [
    "#### After feature engineering, without vandalism_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef5ac88",
   "metadata": {},
   "source": [
    "#### With vandalism_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cee09f",
   "metadata": {},
   "source": [
    "### Comparison of models against baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388f1f92",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Testing results on the balanced and imbalanced test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc72d571",
   "metadata": {},
   "source": [
    "### Comparison of flagship against Cluebot's reported performance (just look at trial-reports maybe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fbd041",
   "metadata": {},
   "source": [
    "## Conclusion/Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb33dad9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

import re
from collections import defaultdict
from typing import Iterable, Set
from sklearn.base import BaseEstimator, TransformerMixin, _fit_context
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import StratifiedKFold
from sklearn.naive_bayes import MultinomialNB





class VandalismScorer(TransformerMixin, BaseEstimator):
    """
    A simple Naive Bayes inspired scorer that estimates the probability
    that a given edit is vandalism based on the words added in that edit.
    Implementation of class based on the template implementation given at
    https://github.com/scikit-learn-contrib/project-template/blob/main/skltemplate/_template.py
    (template file as of June 25, 2025)

    Parameters
    ----------
    smoothing : int, default=1
        Smoothing parameter for Laplace smoothing.
    
    Attributes
    ----------
    vandalism_counts_ : defaultdict(int)
        Dictionary to store counts of words in vandalism edits.
    constructive_counts_ : defaultdict(int)
        Dictionary to store counts of words in constructive edits.
    word_probs_ : dict
        Dictionary to store probabilities of words in vandalism edits.
    
    """

    # This is a dictionary allowing to define the type of parameters.
    # It is used to validate parameters within the `_fit_context` decorator.
    _parameter_constraints = {"smoothing": [int], "n_splits": [int], "fit_prior": [bool], "random_state": [int]}

    def __init__(self, smoothing: int = 1, n_splits: int = 4, random_state = 42, fit_prior=False) -> None:
        """
        Initialize the scorer with Laplace smoothing parameter.
        """
        self.smoothing = smoothing
        self.n_splits = n_splits
        self.random_state = random_state
        self.vectorizer_ = CountVectorizer()
        self.fit_prior = fit_prior
        self.nb_classifiers_split = [MultinomialNB(fit_prior=fit_prior) for _ in range(n_splits)] #n_splits classifiers to be trained on all-but-one-split of training data
        self.nb_classifier_total = MultinomialNB(fit_prior=fit_prior) # classifier to be trained on all of the training data
        self.nb_classifiers = self.nb_classifiers_split + [self.nb_classifier_total]
        self.EditID_to_classifier_index = defaultdict(lambda: n_splits) # defaultdict mapping EditIDs to the classifier trained on subset that doesn't contain this Edit. default value is thet index of nb_classifier_total in nb_classifiers
        self.cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state) # the splits used for training the classifiers in nb_classifiers_split

    @_fit_context(prefer_skip_nested_validation=True)
    def fit(
        self, X, labels
    ):
        """
        Initializes and trains (n_splits + 1) MultinomialNB classifiers on (X, labels).
        n_splits of the MultinomialNB classifiers are trained on all-but-one split of X, which is 
        split using StratifiedKFold. The remaining MultinomialNB classifier is trained on all of X.

        Parameters:
            X: dataset of WP Edits. Must have the columns "added_lines", "deleted_lines" and "EditID"
            labels: Iterable of bools associated to each WP Edit. A value of True indicates vandalism.

        Returns:
            self
        """

        # `_validate_data` is defined in the `BaseEstimator` class.
        # self.X_ = self._validate_data(X.replace(np.nan, ''), accept_sparse=True)
        
        # replace pandas default missing value with empty string for CountVectorizer()
        # reset index to positional index to allow indexing corresponding scipy.spmatrix generated by CountVectorizer()
        self.X_train_ = X.replace(np.nan, '').reset_index(drop=True)
        self.labels_ = labels

        self.vectorizer_.fit(pd.concat([self.X_train_['added_lines'], self.X_train_['deleted_lines']], axis=0)) # build vocabulary from all of the training data
        # X_counts_added = pd.DataFrame.sparse.from_spmatrix(self.vectorizer_.transform(X_transformed['added_lines']), columns=self.vectorizer_.vocabulary_)
        # X_counts_deleted = pd.DataFrame.sparse.from_spmatrix(self.vectorizer_.transform(X_transformed['deleted_lines']), columns=self.vectorizer_.vocabulary_)   
        # X_counts_diff = (X_counts_added - X_counts_deleted).clip(lower=0)
        # not used anymore

        # For memory efficiency, we'll work directly with scipy sparse matrices
        # instead of creating intermediate pandas sparse DataFrames.
        X_counts_added = self.vectorizer_.transform(self.X_train_['added_lines'])
        X_counts_deleted = self.vectorizer_.transform(self.X_train_['deleted_lines'])

        # Subtract matrices and clip at 0 (we only care about added words).
        # .maximum(0) is an efficient way to do this with sparse matrices.
        X_counts_diff = (X_counts_added - X_counts_deleted).maximum(0)
        X_counts_diff.eliminate_zeros()

        # Train nb_classifier_total on all of X_train_
        self.nb_classifier_total.fit(X_counts_diff, self.labels_)

        for i, (train_idx, target_idx) in enumerate(self.cv.split(self.X_train_, self.labels_)):
            Xt = X_counts_diff[train_idx]
            yt = self.labels_.iloc[train_idx]

            Xp = X_counts_diff[target_idx]
            yp = self.labels_.iloc[target_idx]

            for idx in target_idx:
                # dictionary from EditIDs in Xp to i, the index of the nb_classifier_splits to be used to compute vandalism_score for Xp
                self.EditID_to_classifier_index[self.X_train_.iat[idx, self.X_train_.columns.get_loc('EditID')]] = i
            
            # Train nb_classifiers_split[i] on Xt, yt
            self.nb_classifiers_split[i].fit(Xt, yt)
        return self
    
    def transform(
        self, X
    ) -> pd.DataFrame:
        """
        Compute vandalism scores for new edits based on
        learned word probabilities.

        Parameters:
            X: dataset of WP Edits, shape (n_samples, n_features). Must have the columns "added_lines", "deleted_lines" and "EditID".
            n_splits: number of splits to use for training Naive Bayes.

        Returns:
            X_transformed: dataset of WP Edits augmented with pred_proba output from Naive Bayes, shape (n_samples, n_features+1). Adds a column called "vandalism_score".
        """
        X_transformed = X.copy().replace(np.nan, '') # In keeping with sklearn API, we don't want to directly modify the input data
        X_transformed.reset_index(drop=True, inplace=True) # Reset index to positional index to allow accessing the scipy.spmatrix by index
        X_transformed.reset_index(inplace=True) # Store the positional index in a separate column

        X_counts_added = self.vectorizer_.transform(X_transformed['added_lines'])
        X_counts_deleted = self.vectorizer_.transform(X_transformed['deleted_lines'])

        # Subtract matrices and clip at 0 (we only care about added words).
        # .maximum(0) is an efficient way to do this with sparse matrices.
        X_counts_diff = (X_counts_added - X_counts_deleted).maximum(0)
        X_counts_diff.eliminate_zeros()
        
        # initializes classifier_index to self.n_splits if EditID not seen during fit(). Otherwise use EditID_to_classifier_index[EditID]
        X_transformed['classifier_index'] = X_transformed[['EditID']].map(lambda x: self.EditID_to_classifier_index[x])

        X_transformed['vandalism_score'] = X_transformed[['index', 'classifier_index']].apply(lambda x: self.nb_classifiers[x['classifier_index']].predict_proba(X_counts_diff[x['index']])[:, self.nb_classifiers[x['classifier_index']].classes_].item(), axis=1)

        return X_transformed.drop(['added_lines', 'deleted_lines', 'classifier_index', 'index', 'EditID'], axis=1)